tâ€™s concise yet detailed, professional, and includes everything from overview to setup, usage, and roadmap.

# ğŸ§  Ollama Doc Assistant  
*A local, privacy-first AI Q&A app built with Next.js, Ollama (Qwen3:4b), and BM25 document retrieval.*

![Tech Stack](https://img.shields.io/badge/Next.js-13%2B-black?logo=next.js) ![Tailwind](https://img.shields.io/badge/TailwindCSS-3-blue?logo=tailwindcss) ![Ollama](https://img.shields.io/badge/Ollama-Qwen3%3A4b-green) ![License](https://img.shields.io/badge/License-MIT-orange)

---

## ğŸš€ Overview  
**Ollama Doc Assistant** lets you chat with your own documents â€” **entirely offline**.  
It combines a **Next.js frontend**, a **Node.js backend**, **BM25 search** for context retrieval, and **Ollamaâ€™s Qwen3:4b** model for intelligent, local answers.

---

## âœ¨ Features  
- ğŸ¤– **Local AI Assistant** â€” powered by Ollama + Qwen3:4b  
- ğŸ§© **Document Q&A** â€” ask questions directly from your `/docs`  
- âš™ï¸ **BM25 Retrieval** â€” finds the most relevant context for answers  
- ğŸ§¾ **Clean Markdown UI** â€” formatted answers, code highlighting & sources  
- ğŸš€ **Next.js 13 + Tailwind UI** â€” fast, responsive, and extendable  
- ğŸ”’ **100% Private** â€” runs fully on your machine, no external APIs  

---

## ğŸ§  How It Works  
1. You type a question in the web UI.  
2. The app retrieves related text chunks from `/docs` using BM25.  
3. It sends your question + context to **Ollama (Qwen3:4b)** locally.  
4. Ollama generates a context-aware, formatted answer.  
5. The answer is displayed beautifully with markdown & sources.

---

## ğŸ—ï¸ Architecture  


Next.js (frontend + backend API)
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ page.jsx â†’ Main UI page
â”‚ â”œâ”€â”€ api/answer/route.js â†’ Backend route to Ollama
â”‚ â””â”€â”€ components/EnhancedEditor.jsx â†’ Modern Q&A editor
â”‚
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ ingest.js â†’ Builds BM25 index from /docs
â”‚
â”œâ”€â”€ lib/
â”‚ â””â”€â”€ search.js â†’ Performs document search
â”‚
â”œâ”€â”€ docs/ â†’ Place your Markdown/Text files here
â””â”€â”€ data/index.json â†’ Generated BM25 index


---

## âš™ï¸ Installation  

### Prerequisites  
- Node.js **v18+**  
- [Ollama](https://ollama.ai) installed locally  
- Qwen3 model pulled:  
  ```bash
  ollama pull qwen3:4b

Setup
# Clone the repository
git clone https://github.com/YOUR_USERNAME/ollama-doc-assistant.git
cd ollama-doc-assistant

# Install dependencies
npm install

# Build document index
npm run ingest

# Start Ollama in a new terminal
ollama run qwen3:4b

# Start the app
npm run dev


Then open ğŸ‘‰ http://localhost:3000

ğŸ“‚ Folder Structure
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/answer/route.js
â”‚   â”œâ”€â”€ components/EnhancedEditor.jsx
â”‚   â””â”€â”€ page.jsx
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest.js
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ search.js
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ sample.md
â”œâ”€â”€ data/
â”‚   â””â”€â”€ index.json
â””â”€â”€ README.md

âš¡ Environment Variables

Optional .env.local settings:

OLLAMA_URL=http://localhost:11434
OLLAMA_FETCH_TIMEOUT_MS=30000

ğŸ’¬ Example Questions

â€œSummarize architecture.md in 3 bullet points.â€

â€œList all APIs mentioned in my documentation.â€

â€œExplain this JavaScript function step by step.â€

â€œWhat is JWT authentication and how does it work?â€

ğŸ”® Roadmap

 Add chat memory & conversation history

 Enable streaming responses from Ollama

 Support PDF and Docx ingestion

 Integrate embeddings (Chroma / FAISS) for semantic search

 Add dark/light theme toggle

ğŸ¤ Contributing

Pull requests are welcome!
For major changes, open an issue first to discuss what youâ€™d like to improve.

ğŸ‘¨â€ğŸ’» Author

Karthikeyan T

ğŸ’¼ LinkedIn

ğŸ“§ tkarthikeyan@gmail.com

ğŸªª License

Licensed under the MIT License â€” free to use, modify, and distribute.